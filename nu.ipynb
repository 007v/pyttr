{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural TTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is defined in the Python file nu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nu import Type, BType, PType, DepType, Pred, MeetType_n, FunType, InhibitType_n, StringType_n, Ty, iota, gensym_n, nu, and_n, MeetType, or_n, JoinType, labels, Rec, RecType\n",
    "from neurons import Network, Neuron, Synapse, ActivityPattern\n",
    "from utils import show, example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1:  `nu` maps types to types of neural activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types in general correspond to a pattern of activation on a given network. We define a type `T` called `MyType`.  Its corresponding type of neural activation is `nu(T)` which by default has the name `MyType_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyType_n\n"
     ]
    }
   ],
   "source": [
    "T = Type('MyType')\n",
    "Tn = nu(T)\n",
    "print(show(Tn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a network `N` and add a single neuron `n1` to it.  A network is a one dimensional array of neurons. `n1` will be indexed as `[0]` in the network `N`.  We define an activity pattern `h1` which is a two dimensional array indicating which neurons should be activated at each step.  `h1` has only a single step which activates the 0th neuron in a network. We add this activity pattern to the type `Tn` we defined above.  Note that when we add an activity pattern for a type we mention the network for which it holds.  This type may be implemented as different patterns of activation for different networks.  That is, we should not expect to find identical implementations of types in different brains.\n",
    "\n",
    "In order to see what happens in `N` when we it represents this type we use the method `ntrace` to turn on neural tracing.  We then create an instance of the neural type `Tn` on `N` and display `N`'s history.  The neuron has been given a default name of `neuron1`.  First it was off and then in was on.  That is, first there was no signal on its axon and then there was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  -  -\n",
      "neuron0  0  1\n",
      "-------  -  -\n"
     ]
    }
   ],
   "source": [
    "N = Network()\n",
    "n1 = N.add_neuron()\n",
    "h1 = ActivityPattern([[0]])\n",
    "Tn.add_apat(N,h1)\n",
    "N.ntrace()\n",
    "Tn.create_n(N)\n",
    "N.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural types have a method `query_n` which takes a network as argument and tells you whether the history you have recorded using `ntrace` contains an instantiation of the neural type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(Tn.query_n(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2:  Activity patterns can involve more than one neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a neural type for a basic type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyBasicType_n\n"
     ]
    }
   ],
   "source": [
    "T = BType('MyBasicType')\n",
    "Tn = nu(T)\n",
    "print(show(Tn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example `T` corresponds to the activation of two neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  -  -  -\n",
      "neuron1  0  1  1\n",
      "neuron2  0  0  1\n",
      "-------  -  -  -\n"
     ]
    }
   ],
   "source": [
    "N = Network()\n",
    "n1 = N.add_neuron()\n",
    "n2 = N.add_neuron()\n",
    "h1 = ActivityPattern([[0],[1]])\n",
    "Tn.add_apat(N,h1)\n",
    "N.ntrace()\n",
    "Tn.create_n(N)\n",
    "N.display_history()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as before, we can check whether the history instantiates the neural type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(Tn.query_n(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3:  neural types for recursive ptypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TTR 'hug($a$,$b$)' is known a ptype (a type constructed from a predicate together with its arguments).  Intuitively it is a type of situation in which $a$ hugs $b$.\n",
    "\n",
    "Here we use straight pyttr to create a type `Ind`, with name `'Ind'`, to serve as the type of individuals and we use the `judge`-method to assert that `'a'` and `'b'` are of this type.  (We are allowing individuals to be represented by Python strings.)  We declare `hug` to be a predicate, with name `'hug'` which takes two arguments of type `Ind`.  We can now create a ptype with `hug` as predicate and `'a'` and `'b'` as arguments.  Here we set the Python variable `hug_a_b` to this ptype.  We can use `show` to give a not unexpected printout of this ptype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hug(a, b)\n"
     ]
    }
   ],
   "source": [
    "Ind = Type('Ind')\n",
    "Ind.judge('a')\n",
    "Ind.judge('b')\n",
    "hug = Pred('hug',[Ind,Ind])\n",
    "hug_a_b = PType(hug,['a','b'])\n",
    "print(show(hug_a_b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now connect what we have done to neural TTR, starting by creating a network. `iota` (think of $\\iota$ as related to individuals) is a repository of activation patterns corresponding to individuals represented as Python strings.  `iota` has a method `add_grandmother` (as in  \"grandmother neuron\") which will add a single neuron to a network and associate activation of that neuron with the individual in its first argument. \n",
    "\n",
    "We create a type of neural activation, `hug_n`, corresponding to the predicate `hug` and call the `add_grandmother`-method for that type.  This will create a new neuron for the network and make activation of that neuron be the activity pattern associated with the type `hug_n`.  We now have what we need to create a neural type for `hug_a_b` and we set the Python variable `hug_a_b_n` to the neural type.\n",
    "\n",
    "We now turn on neural tracing, create an event of type `hug_a_b_n` on the network and display the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  -  -  -  -  -\n",
      "a       0  0  1  0  0\n",
      "b       0  0  0  1  0\n",
      "hug_n   0  1  0  0  0\n",
      "ptype2  2  1  1  1  0\n",
      "rel     2  1  0  0  0\n",
      "arg0    2  0  1  0  0\n",
      "arg1    2  0  0  1  0\n",
      "------  -  -  -  -  -\n"
     ]
    }
   ],
   "source": [
    "N = Network()\n",
    "iota.add_grandmother('a',N)\n",
    "iota.add_grandmother('b',N)\n",
    "hug_n = nu(hug)\n",
    "hug_n.add_grandmother(N)\n",
    "hug_a_b_n = nu(hug_a_b)\n",
    "\n",
    "N.ntrace()\n",
    "hug_a_b_n.create_n(N)\n",
    "N.display_history()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trace of the history of the network is possibly more complicated than you had been expecting.  The first three rows correspond to the neurons that we added by our calls to the `add_grandmother` methods.  In the first column these three neurons are off (the network is at rest). Then in three successive steps the predicate, the first argument and the second argument are activated.  Finally, in the fifth column, the network is at rest again.  \n",
    "\n",
    "Why did we choose to activate the neurons in order rather than, say, activate all three at the same time?  The reason is that the network needs to be able to distinguish between 'hug($a$,$b$)' and 'hug($b$,$a$)'.  If all three neurons were activated at the same time both of these would have the same representation.  In the literature on neural representation this is often referred to as $\\textit{the binding problem}$.\n",
    "\n",
    "But what is going on in the lower four rows of the history display?  In creating a neural event of the required type, the network discovered that it needed to create more neurons for book-keeping purposes.  In the first column these neurons were not present in the network and this is represented by the occurrences of `2`.  By the time we get to the second column the additional four neurons have been created.  Creating new neurons like this may seem implausible as a model of what happens in an animal brain.  However, it does not seem so implausible that many neurons in an animal brain are available to be designated as having a certain function.  Thus the creation of new neurons in our computational model could be seen as corresponding to the use of previously unused neurons in an animal brain.\n",
    "\n",
    "The new neurons are used for bookkeeping purposes.  How do we know that an event on the network of the type represented in the top three rows is in fact the representation of a ptype rather than just a sequence of three representations: of a predicate, followed by a representation of `'a'` and a separate representation of `'b'`.  The three events are tied together by the activation of the neuron labelled `ptype2` (\"ptype with two arguments\") throughout the duration of the three events.  The role of the individual events in the ptype are indicated by the remaining three neurons, labelled `rel`, `arg0` and `arg1` which are active simultaneously with `hug_n`, `a` and `b` respectively.  This means that the order of the three individual events is of itself not important, although the current implementation will always produce them in the order given here.  They could, however, in principle occur in any order preserving the simultaneity of activation of the role-labelling neurons and the neurons `hug_n`, `a` and `b`.  This, then, represents our approach to the binding problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another central problem for representing the kind of content you need for natural language on a neural architecture is known as $\\textit{the recursion problem}$.  We illustrate this here by allowing predicates to take ptypes as arguments, thus allowing ptypes within ptypes to an arbitrary depth of embedding.\n",
    "\n",
    "First we turn off neural tracing in the network `N`.  This will remove the previous history of the network and no events on the network will be stored until we restart neural tracing.\n",
    "\n",
    "We now extend our environment using pyttr.  We declare a new individual `'c'` and a new predicate `believe` whose first argument is required to be an individual and whose second argument is required to be a type.  (The type `Ty` is provided by pyttr as the type of types, that is, anything defined as a type in pyttr will be of the type `Ty`.)  We then define the ptype `believe_c_hug_a_b` corresponding to believe($c$, hug($a$, $b$)). \n",
    "\n",
    "We now use neural TTR to create a type of neural activation `believe_c_hug_a_b_n` corresponding to this ptype. We add a grandmother neuron for `'c'` to `N`, generate the type of neurological activation `believe_n` for the predicate `believe` and add a grandmother neuron for it to the network.\n",
    "\n",
    "Finally, we turn on neural tracing, create an event of type `believe_c_hug_a_b_n` on the network and display the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.nontrace()\n",
    "\n",
    "\n",
    "Ind.judge('c')\n",
    "believe = Pred('believe',[Ind,Ty])\n",
    "believe_c_hug_a_b = PType(believe,['c',hug_a_b])\n",
    "\n",
    "believe_c_hug_a_b_n = nu(believe_c_hug_a_b)\n",
    "iota.add_grandmother('c',N)\n",
    "believe_n = nu(believe)\n",
    "believe_n.add_grandmother(N)\n",
    "\n",
    "\n",
    "N.ntrace()\n",
    "believe_c_hug_a_b_n.create_n(N)\n",
    "N.display_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a second collection of neurons `ptype2`, `rel`, `arg0` and `arg1` to deal with the fact that we have represented a ptype which has a ptype within it.  Notice that the activation of `arg1` for the `ptype2` whose relation is represented by `believe_n` coincides with the activation of the second `ptype2` whose relation is represented by `hug_n`.\n",
    "\n",
    "We can increase the embedding to an arbitrary depth, 'know($d$, believe($c$, hug($a$,$b$)))' and so on, and with each increase in depth we will generate a new collection of book-keeping neurons.  While for any snapshot of the neural architecture at a point in time it will have what is needed to process a certain finite depth of embedding, there is in principle no limit to the depth of embedding that it can process since it can always expand to account for an extra level of embedding.  In practice, of course, the depth of embedding is limited by the computational resources available (the size of the memory in the computer or the number of neurons in the brain).  This simple technique might provide us with one route into understanding how the plasticity of a brain allows it to devote resources to tasks on an as needed basis.\n",
    "\n",
    "This, then, is our proposal for dealing with the recursion problem. Note that it relies on two aspects of our proposal:  firstly, that informational content is represented as $\\textit{events}$ on a network rather than as architectural structure and secondly, that the network has $\\textit{plasticity}$ in that it can expand or dedicate unused neurons to a particular task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: dependent types as functions with arbitrary depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another place where you need recursion is when you characterize functions, since a function can return another function as result.  In this implementation we only treat functions which are dependent types, that is, they return a type or a function which is a dependent type.  The examples we will deal with here are $\\lambda v\\!:\\!\\textit{Ind}\\ .\\ \\textrm{hug}(v,b)$ and $\\lambda x\\!:\\!\\textit{Ind}\\ .\\ \\lambda y\\!:\\!\\textit{Ind}\\ .\\ \\textrm{hug}(x,y)$.\n",
    "\n",
    "First we use pyttr to create a dependent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = DepType('v',Ind,PType(hug,['v','b']))\n",
    "print(show(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use neural TTR to turn off neural tracing on the network `N`.  We create a type of neural activity corresponding to `Ind` and add a grandmother for it on `N`.  We now create a type of neural activity `Tn` corresponding to the dependent type.\n",
    "\n",
    "We now turn on neural tracing and create an event of the neural type on `N` and display `N`'s history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.nontrace()\n",
    "Ind_n = nu(Ind)\n",
    "Ind_n.add_grandmother(N)\n",
    "Tn = nu(T)\n",
    "\n",
    "N.ntrace()\n",
    "Tn.create_n(N)\n",
    "N.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a collection of book-keeping neurons corresponding to the structure of the function which is the dependent type.  When the neuron `lambda` is active we are in the midst of an event representing a function.  In the first part of the function we represent its domain (`dom`) and its variable (`var`).  These neurons are active simultaneously with the neuron `Ind_n`, encoding the type of the domain of the function. The body of the function is coded by the neuron `rng` (\"range\") which is in fact a two place ptype, as coded by the onset of the first `ptype2` neuron being simultaneous with that of `rng`.  The ptype is coded as before except that the first argument is now indicated to be the variable which was introduced in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to show the kind of activity associated with a neural type on a network is to create an event on the network in the way that we have been doing so far.  Sometimes, however, it is more convenient to inspect the activity pattern which is encoded on the neural type for that particular network.  This is what we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(Tn.show_apat(N))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activity pattern is displayed as a list of lists of triples.  The first element in each triple is the unique identifier of a neuron.  The second element is the convenient name we have assigned the neuron to make our displays human readable.  The third element is either `1` for activate the neuron or `0` for deactivate.  A list of triple thus represents the activations and deactivations which are to be carried out at one particular time step.  The list of lists represents the activations/deactivations to be carried out over a series of time steps.  Note that if a neuron is activated at one time step it will remain active until it is deactivated at a subsequent time step.  Thus in this pattern neuron `14` (`lambda`) is activated in the first time step and is not deactivated until the final time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.nontrace()\n",
    "N.ntrace()\n",
    "T= DepType('x',Ind,DepType('y',Ind,PType(hug,['x','y'])))\n",
    "Tn = nu(T)\n",
    "Tn.create_n(N)\n",
    "N.display_history()\n",
    "pprint(Tn.show_apat(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ppty = FunType(Ind,Ty)\n",
    "every = Pred('every',[Ppty,Ppty])\n",
    "every_n = nu(every)\n",
    "N = Network()\n",
    "every_n.add_grandmother(N)\n",
    "\n",
    "dog = Pred('dog',[Ind])\n",
    "dog_n = nu(dog)\n",
    "dog_n.add_grandmother(N)\n",
    "\n",
    "run = Pred('run',[Ind])\n",
    "run_n = nu(run)\n",
    "run_n.add_grandmother(N)\n",
    "\n",
    "Ind_n.add_grandmother(N)\n",
    "dog_ppty = DepType('x',Ind,PType(dog,['x']))\n",
    "run_ppty = DepType('x',Ind,PType(run,['x']))\n",
    "\n",
    "Tedr = PType(every,[dog_ppty,run_ppty])\n",
    "Tedr_n = nu(Tedr)\n",
    "N.ntrace()\n",
    "Tedr_n.create_n(N)\n",
    "N.display_history()\n",
    "pprint(Tedr_n.show_apat(N))\n",
    "\n",
    "\n",
    "\n",
    "N.nontrace()\n",
    "m = N.memorize_type(Tedr_n,'every dog runs')\n",
    "N.ntrace()\n",
    "m.excite()\n",
    "N.run()\n",
    "N.display_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "Ind_n.add_grandmother(N)\n",
    "iota.add_grandmother('a',N)\n",
    "iota.add_grandmother('b',N)\n",
    "a_n = nu('a')\n",
    "pprint(Ind_n.show_apat(N))\n",
    "pprint(a_n.show_apat(N))\n",
    "pprint(Ind_n.judgmnt_type_n(a_n).show_apat(N))\n",
    "\n",
    "m = N.memorize_judgmnt(Ind_n,a_n,'a:Ind')\n",
    "N.ntrace()\n",
    "m.excite()\n",
    "N.run()\n",
    "N.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uses variables from example 5\n",
    "N = Network()\n",
    "every_n.add_grandmother(N)\n",
    "dog_n.add_grandmother(N)\n",
    "run_n.add_grandmother(N)\n",
    "Ind_n.add_grandmother(N)\n",
    "T = PType(every,[dog_ppty,run_ppty])\n",
    "T_n = nu(T)\n",
    "iota.add_grandmother('e',N)\n",
    "e_n = nu('e')\n",
    "m = N.memorize_judgmnt(T_n,e_n, 'e:every(dog,run)')\n",
    "N.ntrace()\n",
    "m.excite()\n",
    "N.run()\n",
    "N.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "T1 = Type('T1')\n",
    "T2 = Type('T2')\n",
    "T1_n = nu(T1)\n",
    "T2_n = nu(T2)\n",
    "T1_n.add_grandmother(N)\n",
    "T2_n.add_grandmother(N)\n",
    "and_n.add_grandmother(N)\n",
    "iota.add_grandmother('a',N)\n",
    "T3 = MeetType(T1,T2)\n",
    "T3_n = nu(T3)\n",
    "m = N.memorize_judgmnt(T3_n,a_n,'a:T1&T2')\n",
    "N.ntrace()\n",
    "m.excite()\n",
    "N.run()\n",
    "N.display_history()\n",
    "\n",
    "print(N.match_apat(T1_n.judgmnt_type_n(a_n).getapat(N)))\n",
    "print(N.match_apat(T2_n.judgmnt_type_n(a_n).getapat(N)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "T1 = Type('T1')\n",
    "T2 = Type('T2')\n",
    "T1_n = nu(T1)\n",
    "T2_n = nu(T2)\n",
    "T1_n.add_grandmother(N)\n",
    "T2_n.add_grandmother(N)\n",
    "or_n.add_grandmother(N)\n",
    "iota.add_grandmother('a',N)\n",
    "T3 = JoinType(T1,T2)\n",
    "T3_n = nu(T3)\n",
    "m = N.memorize_judgmnt(T3_n,a_n,'a:T1vT2')\n",
    "N.ntrace()\n",
    "m.excite()\n",
    "N.run()\n",
    "N.display_history()\n",
    "\n",
    "print(N.match_apat(T1_n.judgmnt_type_n(a_n).getapat(N)))\n",
    "print(N.match_apat(T2_n.judgmnt_type_n(a_n).getapat(N)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Subtyping for neural types in terms of a relation on apats on a given network.  Works for these examples...\n",
    "print(T1_n.judgmnt_type_n(a_n).subtype_of_n(T3_n.judgmnt_type_n(a_n),N))\n",
    "and_n.add_grandmother(N)\n",
    "T4 = MeetType(T1,T2)\n",
    "T4_n = nu(T4)\n",
    "print(T1_n.judgmnt_type_n(a_n).subtype_of_n(T4_n.judgmnt_type_n(a_n),N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "labels.add_grandmother('l_x',N)\n",
    "labels.add_grandmother('l_e',N) \n",
    "iota.add_grandmother('a',N)\n",
    "iota.add_grandmother('s',N)\n",
    "r = Rec({'l_x':'a','l_e':'s'})\n",
    "r_n = nu(r)\n",
    "pprint(r_n.show_apat(N))\n",
    "\n",
    "N.ntrace()\n",
    "r_n.create_n(N)\n",
    "N.display_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "labels.add_grandmother('l_x',N)\n",
    "labels.add_grandmother('l_e',N)\n",
    "Ind_n.add_grandmother(N)\n",
    "dog_n.add_grandmother(N)\n",
    "Dog = DepType('v',Ind,PType(dog,['v']))\n",
    "T_dog = RecType({'l_x':Ind,\n",
    "                 'l_e':(Dog,['l_x'])})\n",
    "T_dog_n = nu(T_dog)\n",
    "pprint(T_dog_n.show_apat(N))\n",
    "\n",
    "N.ntrace()\n",
    "T_dog_n.create_n(N)\n",
    "N.display_history()\n",
    "#Problem with two labels at same time in dependent fields\n",
    "#Now solved: a label neuron is marked as either a label or part of a value\n",
    "\n",
    "#Random order? np.random.shuffle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function application\n",
    "N = Network()\n",
    "dog_n.add_grandmother(N)\n",
    "Ind_n.add_grandmother(N)\n",
    "Dog = DepType('v',Ind,PType(dog,['v']))\n",
    "iota.add_grandmother('a',N)\n",
    "print(show(Dog.app('a')))\n",
    "print('\\n')\n",
    "Dog_n = nu(Dog)\n",
    "a_n = nu('a')\n",
    "Dog_a_n = nu(Dog.app('a'))\n",
    "pprint(Dog_n.show_apat(N))\n",
    "print('\\n')\n",
    "pprint(a_n.show_apat(N))\n",
    "print('\\n')\n",
    "pprint(Dog_a_n.show_apat(N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Substitution in records\n",
    "N = Network()\n",
    "labels.add_grandmother('l_x',N)\n",
    "labels.add_grandmother('l_e',N) \n",
    "iota.add_grandmother('a',N)\n",
    "iota.add_grandmother('s',N)\n",
    "r = Rec({'l_x':'a','l_e':'s'})\n",
    "iota.add_grandmother('s1',N)\n",
    "r1 = r.subst('s','s1')\n",
    "r1_n = nu(r1)\n",
    "pprint(r1_n.show_apat(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "labels.add_grandmother('l_x',N)\n",
    "labels.add_grandmother('l_e',N)\n",
    "Ind_n.add_grandmother(N)\n",
    "dog_n.add_grandmother(N)\n",
    "cat = Pred('cat',[Ind])\n",
    "cat_n = nu(cat)\n",
    "cat_n.add_grandmother(N)\n",
    "Dog = DepType('v',Ind,PType(dog,['v']))\n",
    "Cat = DepType('v',Ind,PType(cat,['v']))\n",
    "T_dog = RecType({'l_x':Ind,\n",
    "                 'l_e':(Dog,['l_x'])})\n",
    "T_cat = T_dog.subst(Dog,Cat)\n",
    "print(show(T_dog))\n",
    "print(show(T_cat))\n",
    "T_dog_n = nu(T_dog)\n",
    "T_cat_n = nu(T_cat)\n",
    "pprint(T_dog_n.show_apat(N))\n",
    "print('\\n')\n",
    "pprint(T_cat_n.show_apat(N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = Network()\n",
    "labels.add_grandmother('l_x',N)\n",
    "labels.add_grandmother('l_e',N) \n",
    "iota.add_grandmother('a',N)\n",
    "iota.add_grandmother('s',N)\n",
    "Ind_n.add_grandmother(N)\n",
    "r = Rec({'l_x':'a','l_e':'s'})\n",
    "r_n = nu(r)\n",
    "dog_n.add_grandmother(N)\n",
    "Dog = DepType('v',Ind,PType(dog,['v']))\n",
    "T_dog = RecType({'l_x':Ind,\n",
    "                 'l_e':(Dog,['l_x'])})\n",
    "T_dog_n = nu(T_dog)\n",
    "pprint(T_dog_n.resolve(r_n).show_apat(N))\n",
    "print('\\n')\n",
    "j_n = T_dog_n.judgmnt_type_n(r_n)\n",
    "pprint(j_n.show_apat(N))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "N.ntrace()\n",
    "j_n.create_n(N)\n",
    "N.display_history()\n",
    "\n",
    "\n",
    "\n",
    "labels.add_grandmother('l_type',N)\n",
    "labels.add_grandmother('l_obj',N)\n",
    "j_n = nu(T_dog.aus_prop(r))\n",
    "pprint(j_n.show_apat(N))\n",
    "N.nontrace()\n",
    "N.ntrace()\n",
    "j_n.create_n(N)\n",
    "N.display_history()\n",
    "\n",
    "# See ausprop.pdf for an annotated version of the last example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
